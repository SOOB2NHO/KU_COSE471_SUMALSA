# main.py - ë„¤ì´ë²„ APIë¡œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ í›„ ëŒ“ê¸€ ìˆ˜ì§‘ (Selenium ì—†ì´ API ë°©ì‹)

import pandas as pd
import requests
import re
import time

# âœ… ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´
CLIENT_ID = "DkqIEaI_ltBe70Xdm4W6"  # ë³¸ì¸ì˜ API í‚¤ ì…ë ¥
CLIENT_SECRET = "BAA_gdcR17"
HEADERS = {
    "X-Naver-Client-Id": CLIENT_ID,
    "X-Naver-Client-Secret": CLIENT_SECRET
}
NAVER_NEWS_API = "https://openapi.naver.com/v1/search/news.json"

# âœ… ê¸°ì‚¬ URL ìˆ˜ì§‘ í•¨ìˆ˜
def get_news_urls(query="21ëŒ€ ëŒ€ì„ ", max_articles=2000):
    collected_data = []
    for start in range(1, max_articles + 1, 100):
        display = min(100, max_articles - len(collected_data))
        params = {
            "query": query,
            "display": display,
            "start": start,
            "sort": "date"
        }
        res = requests.get(NAVER_NEWS_API, headers=HEADERS, params=params)
        data = res.json()
        for item in data.get("items", []):
            title = re.sub(r"<.*?>", "", item["title"])
            link = item["link"]
            pub_date = item["pubDate"]
            if "n.news.naver.com" in link:
                collected_data.append([title, link, pub_date])
        if len(data.get("items", [])) < 100:
            break
        time.sleep(0.5)
    return collected_data

# âœ… ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì €ì¥ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘
news_output_csv = "filtered_naver_news.csv"
news_data = get_news_urls(query="21ëŒ€ ëŒ€ì„ ", max_articles=2000)
df_news = pd.DataFrame(news_data, columns=["ì œëª©", "ê¸°ì‚¬URL", "ì‘ì„±ì¼"])
df_news.to_csv(news_output_csv, index=False, encoding='utf-8-sig')
print(f"ğŸ“° ê¸°ì‚¬ URL {len(df_news)}ê°œ ì €ì¥ ì™„ë£Œ: {news_output_csv}")

# âœ… ëŒ“ê¸€ ìˆ˜ì§‘ ëŒ€ìƒ íŒŒì¼ ê²½ë¡œ
csv_path = news_output_csv
output_path = "naver_comments_result.csv"

# âœ… CSV ë¡œë“œ ë° URL ì»¬ëŸ¼ ì„ íƒ
df = pd.read_csv(csv_path)
if 'Naverlink' in df.columns:
    urls = df['Naverlink']
elif 'ê¸°ì‚¬URL' in df.columns:
    urls = df['ê¸°ì‚¬URL']
else:
    raise ValueError("CSV íŒŒì¼ì— 'Naverlink' ë˜ëŠ” 'ê¸°ì‚¬URL' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# âœ… ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™” í•¨ìˆ˜
def flatten(l):
    return [item for sublist in l for item in (sublist if isinstance(sublist, list) else [sublist])]

# âœ… ì „ì²´ ëŒ“ê¸€ ì €ì¥ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
total_comments = []

# âœ… ê° ê¸°ì‚¬ë³„ ëŒ“ê¸€ ìˆ˜ì§‘ ë°˜ë³µ
for idx, url in enumerate(urls):
    try:
        print(f"{idx+1}/{len(urls)}ë²ˆì§¸ ê¸°ì‚¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘: {url}")

        oid = url.split("article/")[1].split("/")[0]
        aid = url.split("article/")[1].split("/")[1].split("?")[0]

        page = 1
        all_comments = []

        header = {
            "User-agent": "Mozilla/5.0",
            "referer": url
        }

        while True:
            callback_id = f"jQuery1123{int(time.time() * 1000)}"
            c_url = (
                f"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json"
                f"?ticket=news&templateId=default_society&pool=cbox5&_callback={callback_id}"
                f"&lang=ko&country=KR&objectId=news{oid}%2C{aid}&pageSize=20&indexSize=10"
                f"&listType=OBJECT&pageType=more&page={page}&sort=FAVORITE"
            )

            res = requests.get(c_url, headers=header)
            html = res.text

            if 'comment":' not in html:
                print("ğŸš« ëŒ“ê¸€ ì—†ìŒ ë˜ëŠ” JSON êµ¬ì¡° ì´ìƒ")
                break

            try:
                total_comm = int(html.split('comment":')[1].split(",")[0])
            except Exception as e:
                print("ğŸš« ëŒ“ê¸€ ìˆ˜ íŒŒì‹± ì˜¤ë¥˜:", e)
                break

            matches = re.findall(r'"contents":"(.*?)","userIdNo"', html)
            if not matches:
                break

            all_comments.extend(matches)

            if page * 20 >= total_comm:
                break
            page += 1
            time.sleep(0.1)

        total_comments.append({
            "url": url,
            "ëŒ“ê¸€ ìˆ˜": len(all_comments),
            "ëŒ“ê¸€": flatten(all_comments)
        })

    except Exception as e:
        print(f"â›” ì˜¤ë¥˜ ë°œìƒ: {e}")
        total_comments.append({
            "url": url,
            "ëŒ“ê¸€ ìˆ˜": 0,
            "ëŒ“ê¸€": []
        })

# âœ… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
df_result = pd.DataFrame(total_comments)
df_result.to_csv(output_path, index=False, encoding='utf-8-sig')
print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")