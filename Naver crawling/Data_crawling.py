import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
import time

# CSV íŒŒì¼ ê²½ë¡œ
csv_path = '/Users/hosubin/Desktop/ubuntu_data/Data Science/filtered_naver_news.csv'
output_path = '/Users/hosubin/Desktop/ubuntu_data/Data Science/ë„¤ì´ë²„ë‰´ìŠ¤_ëŒ“ê¸€_ê²°ê³¼.csv'

# CSV ë¡œë“œ ë° ì»¬ëŸ¼ í™•ì¸
df = pd.read_csv(csv_path)
if 'Naverlink' in df.columns:
    urls = df['Naverlink']
elif 'ê¸°ì‚¬URL' in df.columns:
    urls = df['ê¸°ì‚¬URL']
else:
    raise ValueError("CSV íŒŒì¼ì— 'Naverlink' ë˜ëŠ” 'ê¸°ì‚¬URL' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

def flatten(l):
    return [item for sublist in l for item in (sublist if isinstance(sublist, list) else [sublist])]

total_comments = []

for idx, url in enumerate(urls):
    try:
        print(f"{idx+1}/{len(urls)}ë²ˆì§¸ ê¸°ì‚¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘: {url}")

        oid = url.split("article/")[1].split("/")[0]
        aid = url.split("article/")[1].split("/")[1].split("?")[0]

        page = 1
        all_comments = []

        header = {
            "User-agent": "Mozilla/5.0",
            "referer": url
        }

        while True:
            callback_id = f"jQuery1123{int(time.time() * 1000)}"
            c_url = f"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback={callback_id}&lang=ko&country=KR&objectId=news{oid}%2C{aid}&pageSize=20&indexSize=10&listType=OBJECT&pageType=more&page={page}&sort=FAVORITE"

            res = requests.get(c_url, headers=header)
            html = res.text

            # 'comment":'ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
            if 'comment":' not in html:
                print("ğŸš« ëŒ“ê¸€ ì—†ìŒ ë˜ëŠ” JSON êµ¬ì¡° ì´ìƒ")
                break

            try:
                total_comm = int(html.split('comment":')[1].split(",")[0])
            except Exception as e:
                print("ğŸš« ëŒ“ê¸€ ìˆ˜ íŒŒì‹± ì˜¤ë¥˜:", e)
                break

            matches = re.findall(r'"contents":"(.*?)","userIdNo"', html)
            if not matches:
                break

            all_comments.extend(matches)

            if page * 20 >= total_comm:
                break
            page += 1
            time.sleep(0.1)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€

        total_comments.append({
            "url": url,
            "ëŒ“ê¸€ ìˆ˜": len(all_comments),
            "ëŒ“ê¸€": flatten(all_comments)
        })

    except Exception as e:
        print(f"â›” ì˜¤ë¥˜ ë°œìƒ: {e}")
        total_comments.append({
            "url": url,
            "ëŒ“ê¸€ ìˆ˜": 0,
            "ëŒ“ê¸€": []
        })

# CSVë¡œ ì €ì¥
df_result = pd.DataFrame(total_comments)
df_result.to_csv(output_path, index=False, encoding='utf-8-sig')
print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")